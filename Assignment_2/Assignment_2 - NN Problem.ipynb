{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82a5b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mlrose_hiive\n",
    "import pandas as pd\n",
    "# Import sklearn \n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score,classification_report,confusion_matrix, ConfusionMatrixDisplay\n",
    "# Plotting tools\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "# Misc tools\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f279fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the figures bigger\n",
    "plt.rcParams.update({'font.size': 20})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7dcab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "# Load the data\n",
    "heart_df = pd.read_csv('./heart_2.csv')\n",
    "heart_X = heart_df.drop(columns =['target'])\n",
    "heart_y = heart_df['target']\n",
    "\n",
    "# Split the data into 30:70 ratio\n",
    "X_train_heart, X_test_heart, y_train_heart, y_test_heart = train_test_split(heart_X, heart_y,\n",
    "                                                    test_size = 0.3, random_state = 42, stratify = heart_y\n",
    "                                                   )\n",
    "\n",
    "# From before we should perform some scaling first\n",
    "# We will be using a min max scaler to scale the data\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train_heart)\n",
    "X_train_heart = scaler.transform(X_train_heart)\n",
    "X_test_heart = scaler.transform(X_test_heart)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80800a6b",
   "metadata": {},
   "source": [
    "## RHC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a38e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_search_parameters = ({\n",
    "  'learning_rate_init': [0.0001,0.001,0.01,0.1,0.5],\n",
    "  'restarts': [5],\n",
    "  'activation': [mlrose_hiive.neural.activation.relu],\n",
    "})\n",
    "rhc_nn = mlrose_hiive.NNGSRunner(X_train_heart, y_train_heart, \n",
    "                                 X_test_heart, y_test_heart, \n",
    "                                 experiment_name= \"NeuralNetworkRHC\",\n",
    "                                 output_directory=\"nn_rhc_algorithm/\",\n",
    "                                 seed=42, iteration_list =[10], \n",
    "                                 algorithm=mlrose_hiive.random_hill_climb,\n",
    "                                 hidden_layer_sizes=[[100, 100]],\n",
    "                                 grid_search_parameters=grid_search_parameters, \n",
    "                                 grid_search_scorer_method=f1_score,\n",
    "                                 early_stopping = True,\n",
    "                                 max_attempts =100,\n",
    "                                 generate_curves = True,\n",
    "                                 n_jobs=-1,\n",
    "                                 cv=5\n",
    "                                )\n",
    "run_stats, curves, cv_results, best_est = rhc_nn.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32fd20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_est.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4131cd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rhc_nn_epochs = mlrose_hiive.NeuralNetwork(\n",
    "    hidden_nodes = [100,100], activation = 'relu', \n",
    "    algorithm = 'random_hill_climb', \n",
    "    max_iters=100000,\n",
    "    learning_rate =  0.001,\n",
    "    early_stopping = True,\n",
    "    max_attempts = 100,\n",
    "    curve=True,\n",
    "    random_state=42,\n",
    "    restarts = 5,\n",
    ")\n",
    "\n",
    "train_df_rhc = pd.DataFrame([])\n",
    "val_df_rhc = pd.DataFrame([])\n",
    "\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    X_train_train, X_test_train, y_train_train, y_test_train = train_test_split(X_train_heart, y_train_heart,\n",
    "                                                        test_size = 0.2, random_state = (i+1)*i, stratify = y_train_heart\n",
    "                                                       )\n",
    "\n",
    "    rhc_nn_epochs.fit(X_train_train, y_train_train)\n",
    "    train_curve = rhc_nn_epochs.fitness_curve[:,0]\n",
    "    d = {i: train_curve}\n",
    "    loss_df = pd.DataFrame(data=d)\n",
    "    train_df_rhc = pd.concat([train_df_rhc,loss_df],1)\n",
    "    rhc_nn_epochs.fit(X_test_train, y_test_train)\n",
    "    val_curve = rhc_nn_epochs.fitness_curve[:,0]\n",
    "    d = {i: val_curve}\n",
    "    loss_df = pd.DataFrame(data=d)\n",
    "    val_df_rhc = pd.concat([val_df_rhc,loss_df],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04851df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_rhc['Mean'] = train_df_rhc.loc[:, [0,1,2,3,4]].mean(axis = 1)\n",
    "val_df_rhc['Mean'] = val_df_rhc.loc[:, [0,1,2,3,4]].mean(axis = 1)\n",
    "ax = plt.gca()\n",
    "plt.plot(train_df_rhc['Mean'],  label=\"Training Error\")\n",
    "plt.plot(val_df_rhc['Mean'], label ='Validation Error')\n",
    "plt.title('Loss Curves - '+ 'RHC',fontsize = 16)\n",
    "plt.xlabel(\"Epochs\"), plt.ylabel(\"Loss Values\"), plt.legend(loc=\"best\",prop={'size': 14})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec01938",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rhc_nn_epochs = mlrose_hiive.NeuralNetwork(\n",
    "    hidden_nodes = [100,100], activation = 'relu', \n",
    "    algorithm = 'random_hill_climb', \n",
    "    max_iters=100000,\n",
    "    learning_rate =  0.001,\n",
    "    early_stopping = True,\n",
    "    max_attempts = 100,\n",
    "    curve=True,\n",
    "    random_state=42,\n",
    "    restarts = 5,\n",
    ")\n",
    "\n",
    "time_start = time.time()\n",
    "rhc_train = rhc_nn_epochs.fit(X_train_heart, y_train_heart)\n",
    "fit_time = time.time()\n",
    "print(f'fit_time = {fit_time-time_start}')\n",
    "print(classification_report(y_test_heart, rhc_nn_epochs.predict(X_test_heart)))\n",
    "print(classification_report(y_train_heart, rhc_nn_epochs.predict(X_train_heart)))\n",
    "plt.plot(rhc_nn_epochs.fitness_curve[:,0],label='Training')\n",
    "rhc_test = rhc_nn_epochs.fit(X_test_heart, y_test_heart)\n",
    "plt.plot(rhc_nn_epochs.fitness_curve[:,0],label='Validation')\n",
    "plt.legend()\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(10,6)\n",
    "plt.xlabel(\"Epoch\", fontsize=18)\n",
    "plt.ylabel(\"Loss\", fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67efac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rhc_train.predict(X_test_heart)\n",
    "cm = confusion_matrix(y_test_heart, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=['No','Yes'],\n",
    "                             )\n",
    "disp.plot()\n",
    "disp.im_.colorbar.remove()\n",
    "plt.title('RHC - Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043eebb5",
   "metadata": {},
   "source": [
    "## Genetic Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550d11cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "grid_search_ga = {\n",
    "    \n",
    "    \"learning_rate_init\": [0.0001,0.001,0.01,0.1,0.5],\n",
    "    \"activation\": [mlrose_hiive.neural.activation.relu],\n",
    "    \"population_sizes\":[20,50,100,300],\n",
    "    \"mutation_rate\":[0.1, 0.25, 0.5, 0.8], \n",
    "}\n",
    "\n",
    "ga_nn = mlrose_hiive.NNGSRunner(X_train_heart, y_train_heart, \n",
    "                                 X_test_heart, y_test_heart, \n",
    "                                 experiment_name= \"NeuralNetworkGA\",\n",
    "                                 output_directory=\"nn_ga_algorithm/\",\n",
    "                                 seed=42, iteration_list=[100000], \n",
    "                                 algorithm=mlrose_hiive.genetic_alg,\n",
    "                                 hidden_layer_sizes=[[100, 100]],\n",
    "                                 grid_search_parameters=grid_search_ga,\n",
    "                                 grid_search_scorer_method=f1_score,\n",
    "                                 early_stopping = True,\n",
    "                                 max_attempts =100,\n",
    "                                 generate_curves = True,\n",
    "                                 n_jobs=-1,\n",
    "                                 cv=5\n",
    "                                )\n",
    "run_stats_ga, curves_ga, cv_results_ga, best_est_ga = ga_nn.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f295c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_est_ga.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec078e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ga_nn_epochs = mlrose_hiive.NeuralNetwork(\n",
    "    hidden_nodes = [100,100], activation = 'relu', \n",
    "    algorithm = 'genetic_alg', \n",
    "    max_iters=100000, \n",
    "    learning_rate =  0.001,\n",
    "    early_stopping = True,\n",
    "    max_attempts = 100,\n",
    "    curve=True,\n",
    "    random_state=42,\n",
    "    pop_size= 20,\n",
    "    mutation_prob = 0.25,\n",
    ")\n",
    "train_df_ga = pd.DataFrame([])\n",
    "val_df_ga = pd.DataFrame([])\n",
    "\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    X_train_train, X_test_train, y_train_train, y_test_train = train_test_split(X_train_heart, y_train_heart,\n",
    "                                                        test_size = 0.2, random_state = (i+1)*i, stratify = y_train_heart\n",
    "                                                       )\n",
    "\n",
    "    ga_nn_epochs.fit(X_train_train, y_train_train)\n",
    "    train_curve = ga_nn_epochs.fitness_curve[:,0]\n",
    "    d = {i: train_curve}\n",
    "    loss_df = pd.DataFrame(data=d)\n",
    "    train_df_ga = pd.concat([train_df_ga,loss_df],1)\n",
    "    ga_nn_epochs.fit(X_test_train, y_test_train)\n",
    "    val_curve = ga_nn_epochs.fitness_curve[:,0]\n",
    "    d = {i: val_curve}\n",
    "    loss_df = pd.DataFrame(data=d)\n",
    "    val_df_ga = pd.concat([val_df_ga,loss_df],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a835a21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_ga['Mean'] = train_df_ga.loc[:, [0,1,2,3,4]].mean(axis = 1)\n",
    "val_df_ga['Mean'] = val_df_ga.loc[:, [0,1,2,3,4]].mean(axis = 1)\n",
    "ax = plt.gca()\n",
    "plt.plot(train_df_ga['Mean'],  label=\"Training Error\")\n",
    "plt.plot(val_df_ga['Mean'], label ='Validation Error')\n",
    "plt.title('Loss Curves - '+ 'GA',fontsize = 16)\n",
    "plt.xlabel(\"Epochs\"), plt.ylabel(\"Loss Values\"), plt.legend(loc=\"best\",prop={'size': 14})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321ece7a",
   "metadata": {},
   "source": [
    "## Individual HPO - GA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca768fd3",
   "metadata": {},
   "source": [
    "### Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c2cda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ga_nn_epochs = mlrose_hiive.NeuralNetwork(\n",
    "    hidden_nodes = [100,100], activation = 'relu', \n",
    "    algorithm = 'genetic_alg', \n",
    "    max_iters=100000, \n",
    "    learning_rate =  0.001,\n",
    "    early_stopping = True,\n",
    "    max_attempts = 100,\n",
    "    curve=True,\n",
    "    random_state=42,\n",
    "    pop_size= 20,\n",
    "    mutation_prob = 0.25,\n",
    ")\n",
    "train_df_ga_pop_20 = pd.DataFrame([])\n",
    "val_df_ga_pop_20 = pd.DataFrame([])\n",
    "\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    X_train_train, X_test_train, y_train_train, y_test_train = train_test_split(X_train_heart, y_train_heart,\n",
    "                                                        test_size = 0.2, random_state = (i+1)*i, stratify = y_train_heart\n",
    "                                                       )\n",
    "\n",
    "    ga_nn_epochs.fit(X_train_train, y_train_train)\n",
    "    train_curve = ga_nn_epochs.fitness_curve[:,0]\n",
    "    d = {i: train_curve}\n",
    "    loss_df = pd.DataFrame(data=d)\n",
    "    train_df_ga_pop_20 = pd.concat([train_df_ga_pop_20,loss_df],1)\n",
    "    ga_nn_epochs.fit(X_test_train, y_test_train)\n",
    "    val_curve = ga_nn_epochs.fitness_curve[:,0]\n",
    "    d = {i: val_curve}\n",
    "    loss_df = pd.DataFrame(data=d)\n",
    "    val_df_ga_pop_20 = pd.concat([val_df_ga_pop_20,loss_df],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be2061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ga_nn_epochs = mlrose_hiive.NeuralNetwork(\n",
    "    hidden_nodes = [100,100], activation = 'relu', \n",
    "    algorithm = 'genetic_alg', \n",
    "    max_iters=100000, \n",
    "    learning_rate =  0.001,\n",
    "    early_stopping = True,\n",
    "    max_attempts = 100,\n",
    "    curve=True,\n",
    "    random_state=42,\n",
    "    pop_size= 50,\n",
    "    mutation_prob = 0.25,\n",
    ")\n",
    "train_df_ga_pop_50 = pd.DataFrame([])\n",
    "val_df_ga_pop_50 = pd.DataFrame([])\n",
    "\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    X_train_train, X_test_train, y_train_train, y_test_train = train_test_split(X_train_heart, y_train_heart,\n",
    "                                                        test_size = 0.2, random_state = (i+1)*i, stratify = y_train_heart\n",
    "                                                       )\n",
    "\n",
    "    ga_nn_epochs.fit(X_train_train, y_train_train)\n",
    "    train_curve = ga_nn_epochs.fitness_curve[:,0]\n",
    "    d = {i: train_curve}\n",
    "    loss_df = pd.DataFrame(data=d)\n",
    "    train_df_ga_pop_50 = pd.concat([train_df_ga_pop_50,loss_df],1)\n",
    "    ga_nn_epochs.fit(X_test_train, y_test_train)\n",
    "    val_curve = ga_nn_epochs.fitness_curve[:,0]\n",
    "    d = {i: val_curve}\n",
    "    loss_df = pd.DataFrame(data=d)\n",
    "    val_df_ga_pop_50 = pd.concat([val_df_ga_pop_50,loss_df],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e264bdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ga_nn_epochs = mlrose_hiive.NeuralNetwork(\n",
    "    hidden_nodes = [100,100], activation = 'relu', \n",
    "    algorithm = 'genetic_alg', \n",
    "    max_iters=100000, \n",
    "    learning_rate =  0.001,\n",
    "    early_stopping = True,\n",
    "    max_attempts = 100,\n",
    "    curve=True,\n",
    "    random_state=42,\n",
    "    pop_size= 100,\n",
    "    mutation_prob = 0.25,\n",
    ")\n",
    "train_df_ga_pop_100 = pd.DataFrame([])\n",
    "val_df_ga_pop_100 = pd.DataFrame([])\n",
    "\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    X_train_train, X_test_train, y_train_train, y_test_train = train_test_split(X_train_heart, y_train_heart,\n",
    "                                                        test_size = 0.2, random_state = (i+1)*i, stratify = y_train_heart\n",
    "                                                       )\n",
    "\n",
    "    ga_nn_epochs.fit(X_train_train, y_train_train)\n",
    "    train_curve = ga_nn_epochs.fitness_curve[:,0]\n",
    "    d = {i: train_curve}\n",
    "    loss_df = pd.DataFrame(data=d)\n",
    "    train_df_ga_pop_100 = pd.concat([train_df_ga_pop_100,loss_df],1)\n",
    "    ga_nn_epochs.fit(X_test_train, y_test_train)\n",
    "    val_curve = ga_nn_epochs.fitness_curve[:,0]\n",
    "    d = {i: val_curve}\n",
    "    loss_df = pd.DataFrame(data=d)\n",
    "    val_df_ga_pop_100 = pd.concat([val_df_ga_pop_100,loss_df],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f988acf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ga_nn_epochs = mlrose_hiive.NeuralNetwork(\n",
    "    hidden_nodes = [100,100], activation = 'relu', \n",
    "    algorithm = 'genetic_alg', \n",
    "    max_iters=100000, \n",
    "    learning_rate =  0.001,\n",
    "    early_stopping = True,\n",
    "    max_attempts = 100,\n",
    "    curve=True,\n",
    "    random_state=42,\n",
    "    pop_size= 300,\n",
    "    mutation_prob = 0.25,\n",
    ")\n",
    "train_df_ga_pop_300 = pd.DataFrame([])\n",
    "val_df_ga_pop_300 = pd.DataFrame([])\n",
    "\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    X_train_train, X_test_train, y_train_train, y_test_train = train_test_split(X_train_heart, y_train_heart,\n",
    "                                                        test_size = 0.2, random_state = (i+1)*i, stratify = y_train_heart\n",
    "                                                       )\n",
    "\n",
    "    ga_nn_epochs.fit(X_train_train, y_train_train)\n",
    "    train_curve = ga_nn_epochs.fitness_curve[:,0]\n",
    "    d = {i: train_curve}\n",
    "    loss_df = pd.DataFrame(data=d)\n",
    "    train_df_ga_pop_300 = pd.concat([train_df_ga_pop_300,loss_df],1)\n",
    "    ga_nn_epochs.fit(X_test_train, y_test_train)\n",
    "    val_curve = ga_nn_epochs.fitness_curve[:,0]\n",
    "    d = {i: val_curve}\n",
    "    loss_df = pd.DataFrame(data=d)\n",
    "    val_df_ga_pop_300 = pd.concat([val_df_ga_pop_300,loss_df],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd53fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_ga_pop_20['Mean'] = train_df_ga_pop_20.loc[:, [0,1,2,3,4]].mean(axis = 1)\n",
    "val_df_ga_pop_20['Mean'] = val_df_ga_pop_20.loc[:, [0,1,2,3,4]].mean(axis = 1)\n",
    "\n",
    "train_df_ga_pop_50['Mean'] = train_df_ga_pop_50.loc[:, [0,1,2,3,4]].mean(axis = 1)\n",
    "val_df_ga_pop_50['Mean'] = val_df_ga_pop_50.loc[:, [0,1,2,3,4]].mean(axis = 1)\n",
    "\n",
    "train_df_ga_pop_100['Mean'] = train_df_ga_pop_100.loc[:, [0,1,2,3,4]].mean(axis = 1)\n",
    "val_df_ga_pop_100['Mean'] = val_df_ga_pop_100.loc[:, [0,1,2,3,4]].mean(axis = 1)\n",
    "\n",
    "train_df_ga_pop_300['Mean'] = train_df_ga_pop_300.loc[:, [0,1,2,3,4]].mean(axis = 1)\n",
    "val_df_ga_pop_300['Mean'] = val_df_ga_pop_300.loc[:, [0,1,2,3,4]].mean(axis = 1)\n",
    "\n",
    "plt.plot(train_df_ga_pop_20['Mean'],  label=\"POP 20\", color = 'blue')\n",
    "#plt.plot(val_df_ga_pop_20['Mean'], '--',  label=\"POP 20 - Val\", color = 'blue')\n",
    "plt.plot(train_df_ga_pop_50['Mean'], label ='POP 50', color = 'orange')\n",
    "#plt.plot(val_df_ga_pop_50['Mean'], '--',  label=\"POP 50 - Val\", color = 'orange')\n",
    "plt.plot(train_df_ga_pop_100['Mean'], label ='POP 100', color = 'green')\n",
    "#plt.plot(val_df_ga_pop_100['Mean'], '--',  label=\"POP 100 - Val\", color = 'green')\n",
    "plt.plot(train_df_ga_pop_300['Mean'], label ='POP 300', color = 'red')\n",
    "#plt.plot(val_df_ga_pop_300['Mean'], '--',  label=\"POP 300 - Val\", color = 'red')\n",
    "\n",
    "plt.xlabel(\"Epochs\"), plt.ylabel(\"Loss Values\"), plt.legend(loc=\"best\",prop={'size': 14})\n",
    "plt.title('Varying Population - Fitness Scores', size = 16)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7863c3b",
   "metadata": {},
   "source": [
    "### Mutation Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b8dd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ga_nn_epochs = mlrose_hiive.NeuralNetwork(\n",
    "    hidden_nodes = [100,100], activation = 'relu', \n",
    "    algorithm = 'genetic_alg', \n",
    "    max_iters=100000, \n",
    "    learning_rate =  0.001,\n",
    "    early_stopping = True,\n",
    "    max_attempts = 100,\n",
    "    curve=True,\n",
    "    random_state=42,\n",
    "    pop_size= 20,\n",
    "    mutation_prob = 0.1,\n",
    ")\n",
    "train_df_ga_mut_01 = pd.DataFrame([])\n",
    "val_df_ga_mut_01 = pd.DataFrame([])\n",
    "\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    X_train_train, X_test_train, y_train_train, y_test_train = train_test_split(X_train_heart, y_train_heart,\n",
    "                                                        test_size = 0.2, random_state = (i+1)*i, stratify = y_train_heart\n",
    "                                                       )\n",
    "\n",
    "    ga_nn_epochs.fit(X_train_train, y_train_train)\n",
    "    train_curve = ga_nn_epochs.fitness_curve[:,0]\n",
    "    d = {i: train_curve}\n",
    "    loss_df = pd.DataFrame(data=d)\n",
    "    train_df_ga_mut_01 = pd.concat([train_df_ga_mut_01,loss_df],1)\n",
    "    ga_nn_epochs.fit(X_test_train, y_test_train)\n",
    "    val_curve = ga_nn_epochs.fitness_curve[:,0]\n",
    "    d = {i: val_curve}\n",
    "    loss_df = pd.DataFrame(data=d)\n",
    "    val_df_ga_mut_01 = pd.concat([val_df_ga_mut_01,loss_df],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3473a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ga_nn_epochs = mlrose_hiive.NeuralNetwork(\n",
    "    hidden_nodes = [100,100], activation = 'relu', \n",
    "    algorithm = 'genetic_alg', \n",
    "    max_iters=100000, \n",
    "    learning_rate =  0.001,\n",
    "    early_stopping = True,\n",
    "    max_attempts = 100,\n",
    "    curve=True,\n",
    "    random_state=42,\n",
    "    pop_size= 20,\n",
    "    mutation_prob = 0.25,\n",
    ")\n",
    "train_df_ga_mut_025 = pd.DataFrame([])\n",
    "val_df_ga_mut_025 = pd.DataFrame([])\n",
    "\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    X_train_train, X_test_train, y_train_train, y_test_train = train_test_split(X_train_heart, y_train_heart,\n",
    "                                                        test_size = 0.2, random_state = (i+1)*i, stratify = y_train_heart\n",
    "                                                       )\n",
    "\n",
    "    ga_nn_epochs.fit(X_train_train, y_train_train)\n",
    "    train_curve = ga_nn_epochs.fitness_curve[:,0]\n",
    "    d = {i: train_curve}\n",
    "    loss_df = pd.DataFrame(data=d)\n",
    "    train_df_ga_mut_025 = pd.concat([train_df_ga_mut_025,loss_df],1)\n",
    "    ga_nn_epochs.fit(X_test_train, y_test_train)\n",
    "    val_curve = ga_nn_epochs.fitness_curve[:,0]\n",
    "    d = {i: val_curve}\n",
    "    loss_df = pd.DataFrame(data=d)\n",
    "    val_df_ga_mut_025 = pd.concat([val_df_ga_mut_025,loss_df],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9419bd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ga_nn_epochs = mlrose_hiive.NeuralNetwork(\n",
    "    hidden_nodes = [100,100], activation = 'relu', \n",
    "    algorithm = 'genetic_alg', \n",
    "    max_iters=100000, \n",
    "    learning_rate =  0.001,\n",
    "    early_stopping = True,\n",
    "    max_attempts = 100,\n",
    "    curve=True,\n",
    "    random_state=42,\n",
    "    pop_size= 20,\n",
    "    mutation_prob = 0.5,\n",
    ")\n",
    "train_df_ga_mut_050 = pd.DataFrame([])\n",
    "val_df_ga_mut_050 = pd.DataFrame([])\n",
    "\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    X_train_train, X_test_train, y_train_train, y_test_train = train_test_split(X_train_heart, y_train_heart,\n",
    "                                                        test_size = 0.2, random_state = (i+1)*i, stratify = y_train_heart\n",
    "                                                       )\n",
    "\n",
    "    ga_nn_epochs.fit(X_train_train, y_train_train)\n",
    "    train_curve = ga_nn_epochs.fitness_curve[:,0]\n",
    "    d = {i: train_curve}\n",
    "    loss_df = pd.DataFrame(data=d)\n",
    "    train_df_ga_mut_050 = pd.concat([train_df_ga_mut_050,loss_df],1)\n",
    "    ga_nn_epochs.fit(X_test_train, y_test_train)\n",
    "    val_curve = ga_nn_epochs.fitness_curve[:,0]\n",
    "    d = {i: val_curve}\n",
    "    loss_df = pd.DataFrame(data=d)\n",
    "    val_df_ga_mut_050 = pd.concat([val_df_ga_mut_050,loss_df],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61c25d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ga_nn_epochs = mlrose_hiive.NeuralNetwork(\n",
    "    hidden_nodes = [100,100], activation = 'relu', \n",
    "    algorithm = 'genetic_alg', \n",
    "    max_iters=100000, \n",
    "    learning_rate =  0.001,\n",
    "    early_stopping = True,\n",
    "    max_attempts = 100,\n",
    "    curve=True,\n",
    "    random_state=42,\n",
    "    pop_size= 20,\n",
    "    mutation_prob = 0.8,\n",
    ")\n",
    "train_df_ga_mut_080 = pd.DataFrame([])\n",
    "val_df_ga_mut_080 = pd.DataFrame([])\n",
    "\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    X_train_train, X_test_train, y_train_train, y_test_train = train_test_split(X_train_heart, y_train_heart,\n",
    "                                                        test_size = 0.2, random_state = (i+1)*i, stratify = y_train_heart\n",
    "                                                       )\n",
    "\n",
    "    ga_nn_epochs.fit(X_train_train, y_train_train)\n",
    "    train_curve = ga_nn_epochs.fitness_curve[:,0]\n",
    "    d = {i: train_curve}\n",
    "    loss_df = pd.DataFrame(data=d)\n",
    "    train_df_ga_mut_080 = pd.concat([train_df_ga_mut_080,loss_df],1)\n",
    "    ga_nn_epochs.fit(X_test_train, y_test_train)\n",
    "    val_curve = ga_nn_epochs.fitness_curve[:,0]\n",
    "    d = {i: val_curve}\n",
    "    loss_df = pd.DataFrame(data=d)\n",
    "    val_df_ga_mut_080 = pd.concat([val_df_ga_mut_080,loss_df],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f99ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_ga_mut_01['Mean'] = train_df_ga_mut_01.loc[:, [0,1,2,3,4]].mean(axis = 1)\n",
    "val_df_ga_mut_01['Mean'] = val_df_ga_mut_01.loc[:, [0,1,2,3,4]].mean(axis = 1)\n",
    "\n",
    "train_df_ga_mut_025['Mean'] = train_df_ga_mut_025.loc[:, [0,1,2,3,4]].mean(axis = 1)\n",
    "val_df_ga_mut_025['Mean'] = val_df_ga_mut_025.loc[:, [0,1,2,3,4]].mean(axis = 1)\n",
    "\n",
    "train_df_ga_mut_050['Mean'] = train_df_ga_mut_050.loc[:, [0,1,2,3,4]].mean(axis = 1)\n",
    "val_df_ga_mut_050['Mean'] = val_df_ga_mut_050.loc[:, [0,1,2,3,4]].mean(axis = 1)\n",
    "\n",
    "train_df_ga_mut_080['Mean'] = train_df_ga_mut_080.loc[:, [0,1,2,3,4]].mean(axis = 1)\n",
    "val_df_ga_mut_080['Mean'] = val_df_ga_mut_080.loc[:, [0,1,2,3,4]].mean(axis = 1)\n",
    "\n",
    "plt.plot(train_df_ga_mut_01['Mean'],  label=\"MUT 0.10\")\n",
    "plt.plot(train_df_ga_mut_025['Mean'], label ='MUT 0.25')\n",
    "plt.plot(train_df_ga_mut_050['Mean'], label ='MUT 0.50')\n",
    "plt.plot(train_df_ga_mut_080['Mean'], label ='MUT 0.80')\n",
    "\n",
    "plt.xlabel(\"Epochs\"), plt.ylabel(\"Loss Values\"), plt.legend(loc=\"best\",prop={'size': 14})\n",
    "plt.title('Varying Mutation - Fitness Scores', size = 16)\n",
    "\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee64ee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "ga_train = ga_nn_epochs.fit(X_train, y_train)\n",
    "y_pred = ga_train.predict(X_test_heart)\n",
    "cm = confusion_matrix(y_test_heart, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=['No','Yes'],\n",
    "                             )\n",
    "disp.plot()\n",
    "disp.im_.colorbar.remove()\n",
    "plt.title('GA - Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4945e682",
   "metadata": {},
   "source": [
    "## Simulated Annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e3c7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "grid_search_sa = {\n",
    "    \"learning_rate_init\": [0.0001,0.001,0.01,0.1,0.5],\n",
    "    \"activation\": [mlrose_hiive.neural.activation.relu],\n",
    "    \"schedule\":[mlrose_hiive.GeomDecay(1), mlrose_hiive.GeomDecay(50), mlrose_hiive.GeomDecay(100),\n",
    "                 mlrose_hiive.GeomDecay(250), mlrose_hiive.GeomDecay(300),\n",
    "                 mlrose_hiive.ExpDecay(1), mlrose_hiive.ExpDecay(50), mlrose_hiive.ExpDecay(100),\n",
    "                 mlrose_hiive.ExpDecay(250), mlrose_hiive.ExpDecay(300)\n",
    "                                          ],\n",
    "}\n",
    "\n",
    "sa_nn = mlrose_hiive.NNGSRunner(X_train_heart, y_train_heart, \n",
    "                                 X_test_heart, y_test_heart, \n",
    "                                 experiment_name= \"NeuralNetworkSA\",\n",
    "                                 output_directory=\"nn_sa_algorithm/\",\n",
    "                                 seed=42, iteration_list=[100000], \n",
    "                                 algorithm=mlrose_hiive.algorithms.sa.simulated_annealing,\n",
    "                                 hidden_layer_sizes=[[100, 100]],\n",
    "                                 grid_search_parameters=grid_search_sa,\n",
    "                                 grid_search_scorer_method=f1_score,\n",
    "                                 early_stopping = True,\n",
    "                                 max_attempts =100,\n",
    "                                 generate_curves = True,\n",
    "                                 n_jobs=-1,\n",
    "                                 cv=5\n",
    "                                )\n",
    "run_stats_sa, curves_sa, cv_results_sa, best_est_sa = sa_nn.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cfcdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_est_sa.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c7166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sa_nn_epochs = mlrose_hiive.NeuralNetwork(\n",
    "    hidden_nodes = [100,100], activation = 'relu', \n",
    "    algorithm = 'simulated_annealing', \n",
    "    max_iters=100000,\n",
    "    bias = True,\n",
    "    is_classifier = True, \n",
    "    learning_rate =  0.1,\n",
    "    early_stopping = True,\n",
    "    max_attempts = 100,\n",
    "    curve=True,\n",
    "    random_state=42,\n",
    "    schedule=mlrose_hiive.GeomDecay(1)\n",
    ")\n",
    "\n",
    "time_start = time.time()\n",
    "sa_train = sa_nn_epochs.fit(X_train_heart, y_train_heart)\n",
    "fit_time = time.time()\n",
    "print(f'fit_time = {fit_time-time_start}')\n",
    "print(classification_report(y_test_heart, sa_nn_epochs.predict(X_test_heart)))\n",
    "print(classification_report(y_train_heart, sa_nn_epochs.predict(X_train_heart)))\n",
    "plt.plot(sa_nn_epochs.fitness_curve[:,0],label='Training')\n",
    "sa_test = sa_nn_epochs.fit(X_test_heart, y_test_heart)\n",
    "plt.plot(sa_nn_epochs.fitness_curve[:,0],label='Validation')\n",
    "plt.legend()\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(10,6)\n",
    "plt.xlabel(\"Epoch\", fontsize=18)\n",
    "plt.ylabel(\"Loss\", fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf90c6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sa_train.predict(X_test_heart)\n",
    "cm = confusion_matrix(y_test_heart, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=['No','Yes'],\n",
    "                             )\n",
    "disp.plot()\n",
    "disp.im_.colorbar.remove()\n",
    "plt.title('SA - Confusion Matrix')\n",
    "plt.show()\n",
    "print(classification_report(y_test_heart, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf5aea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sa_nn_epochs = mlrose_hiive.NeuralNetwork(\n",
    "    hidden_nodes = [100,100], activation = 'relu', \n",
    "    algorithm = 'simulated_annealing', \n",
    "    max_iters=100000,\n",
    "    bias = True,\n",
    "    is_classifier = True, \n",
    "    learning_rate =  0.1,\n",
    "    early_stopping = True,\n",
    "    max_attempts = 100,\n",
    "    curve=True,\n",
    "    random_state=42,\n",
    "    schedule=mlrose_hiive.GeomDecay(1)\n",
    ")\n",
    "\n",
    "train_df_sa = pd.DataFrame([])\n",
    "val_df_sa = pd.DataFrame([])\n",
    "\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    X_train_train, X_test_train, y_train_train, y_test_train = train_test_split(X_train_heart, y_train_heart,\n",
    "                                                        test_size = 0.2, random_state = (i+1)*i, stratify = y_train_heart\n",
    "                                                       )\n",
    "\n",
    "    sa_nn_epochs.fit(X_train_train, y_train_train)\n",
    "    train_curve = sa_nn_epochs.fitness_curve[:,0]\n",
    "    d = {i: train_curve}\n",
    "    loss_df_train = pd.DataFrame(data=d)\n",
    "    train_df_sa = pd.concat([train_df_sa,loss_df_train],1)\n",
    "    sa_nn_epochs.fit(X_test_train, y_test_train)\n",
    "    val_curve = sa_nn_epochs.fitness_curve[:,0]\n",
    "    d = {i: val_curve}\n",
    "    loss_df_val = pd.DataFrame(data=d)\n",
    "    val_df_sa = pd.concat([val_df_sa,loss_df_val],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfa0204",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_sa['Mean'] = train_df_sa.loc[:, [0,1,2,3,4]].mean(axis = 1)\n",
    "val_df_sa['Mean'] = val_df_sa.loc[:, [0,1,2,3,4]].mean(axis = 1)\n",
    "\n",
    "ax = plt.gca()\n",
    "plt.plot(train_df_sa['Mean'],  label=\"Training Error\")\n",
    "plt.plot(val_df_sa['Mean'], label ='Validation Error')\n",
    "plt.title('Loss Curves - '+ 'SA',fontsize = 16)\n",
    "plt.xlabel(\"Epochs\"), plt.ylabel(\"Loss Values\"), plt.legend(loc=\"best\",prop={'size': 14})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40542e81",
   "metadata": {},
   "source": [
    "## Individual HPO - SA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb8bf0c",
   "metadata": {},
   "source": [
    "### Decay Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fd505c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sa_nn_epochs = mlrose_hiive.NeuralNetwork(\n",
    "    hidden_nodes = [100,100], activation = 'relu', \n",
    "    algorithm = 'simulated_annealing', \n",
    "    max_iters=100000,\n",
    "    bias = True,\n",
    "    is_classifier = True, \n",
    "    learning_rate =  0.1,\n",
    "    early_stopping = True,\n",
    "    max_attempts = 100,\n",
    "    curve=True,\n",
    "    random_state=42,\n",
    "    schedule=mlrose_hiive.ExpDecay(1)\n",
    ")\n",
    "\n",
    "time_start = time.time()\n",
    "sa_nn_epochs.fit(X_train_heart, y_train_heart)\n",
    "fit_time = time.time()\n",
    "print(f'fit_time = {fit_time-time_start}')\n",
    "print(classification_report(y_test_heart, sa_nn_epochs.predict(X_test_heart)))\n",
    "print(classification_report(y_train_heart, sa_nn_epochs.predict(X_train_heart)))\n",
    "plt.plot(sa_nn_epochs.fitness_curve[:,0],label='Training')\n",
    "sa_train_exp_1 = sa_nn_epochs.fitness_curve[:,0]\n",
    "sa_nn_epochs.fit(X_test_heart, y_test_heart)\n",
    "plt.plot(sa_nn_epochs.fitness_curve[:,0],label='Validation')\n",
    "sa_test_exp_1 = sa_nn_epochs.fitness_curve[:,0]\n",
    "plt.legend()\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(10,6)\n",
    "plt.xlabel(\"Epoch\", fontsize=18)\n",
    "plt.ylabel(\"Loss\", fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da3b883",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sa_nn_epochs = mlrose_hiive.NeuralNetwork(\n",
    "    hidden_nodes = [100,100], activation = 'relu', \n",
    "    algorithm = 'simulated_annealing', \n",
    "    max_iters=100000,\n",
    "    bias = True,\n",
    "    is_classifier = True, \n",
    "    learning_rate =  0.1,\n",
    "    early_stopping = True,\n",
    "    max_attempts = 100,\n",
    "    curve=True,\n",
    "    random_state=42,\n",
    "    schedule=mlrose_hiive.GeomDecay(1)\n",
    ")\n",
    "\n",
    "time_start = time.time()\n",
    "sa_nn_epochs.fit(X_train_heart, y_train_heart)\n",
    "fit_time = time.time()\n",
    "print(f'fit_time = {fit_time-time_start}')\n",
    "print(classification_report(y_test_heart, sa_nn_epochs.predict(X_test_heart)))\n",
    "print(classification_report(y_train_heart, sa_nn_epochs.predict(X_train_heart)))\n",
    "plt.plot(sa_nn_epochs.fitness_curve[:,0],label='Training')\n",
    "sa_train_geo_1 = sa_nn_epochs.fitness_curve[:,0]\n",
    "sa_nn_epochs.fit(X_test_heart, y_test_heart)\n",
    "plt.plot(sa_nn_epochs.fitness_curve[:,0],label='Validation')\n",
    "sa_test_geo_1 = sa_nn_epochs.fitness_curve[:,0]\n",
    "plt.legend()\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(10,6)\n",
    "plt.xlabel(\"Epoch\", fontsize=18)\n",
    "plt.ylabel(\"Loss\", fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5030156a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sa_train_geo_1,  label=\"Decay = Geo\")\n",
    "plt.plot(sa_train_exp_1, label ='Decay = Exp')\n",
    "\n",
    "\n",
    "plt.xlabel(\"Epochs\"), plt.ylabel(\"Loss Values\"), plt.legend(loc=\"best\",prop={'size': 14})\n",
    "plt.title('Varying Schedules - Fitness Scores', size = 16)\n",
    "plt.ylim([0.1,0.4])\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b5b9f9",
   "metadata": {},
   "source": [
    "### Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fd1a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sa_nn_epochs = mlrose_hiive.NeuralNetwork(\n",
    "    hidden_nodes = [100,100], activation = 'relu', \n",
    "    algorithm = 'simulated_annealing', \n",
    "    max_iters=100000,\n",
    "    bias = True,\n",
    "    is_classifier = True, \n",
    "    learning_rate =  0.1,\n",
    "    early_stopping = True,\n",
    "    max_attempts = 100,\n",
    "    curve=True,\n",
    "    random_state=42,\n",
    "    schedule=mlrose_hiive.GeomDecay(50)\n",
    ")\n",
    "\n",
    "time_start = time.time()\n",
    "sa_nn_epochs.fit(X_train_heart, y_train_heart)\n",
    "fit_time = time.time()\n",
    "print(f'fit_time = {fit_time-time_start}')\n",
    "print(classification_report(y_test_heart, sa_nn_epochs.predict(X_test_heart)))\n",
    "print(classification_report(y_train_heart, sa_nn_epochs.predict(X_train_heart)))\n",
    "plt.plot(sa_nn_epochs.fitness_curve[:,0],label='Training')\n",
    "sa_train_geo_50 = sa_nn_epochs.fitness_curve[:,0]\n",
    "sa_nn_epochs.fit(X_test_heart, y_test_heart)\n",
    "plt.plot(sa_nn_epochs.fitness_curve[:,0],label='Validation')\n",
    "sa_test_geo_50 = sa_nn_epochs.fitness_curve[:,0]\n",
    "plt.legend()\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(10,6)\n",
    "plt.xlabel(\"Epoch\", fontsize=18)\n",
    "plt.ylabel(\"Loss\", fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aeb92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sa_nn_epochs = mlrose_hiive.NeuralNetwork(\n",
    "    hidden_nodes = [100,100], activation = 'relu', \n",
    "    algorithm = 'simulated_annealing', \n",
    "    max_iters=100000,\n",
    "    bias = True,\n",
    "    is_classifier = True, \n",
    "    learning_rate =  0.1,\n",
    "    early_stopping = True,\n",
    "    max_attempts = 100,\n",
    "    curve=True,\n",
    "    random_state=42,\n",
    "    schedule=mlrose_hiive.GeomDecay(100)\n",
    ")\n",
    "\n",
    "time_start = time.time()\n",
    "sa_nn_epochs.fit(X_train_heart, y_train_heart)\n",
    "fit_time = time.time()\n",
    "print(f'fit_time = {fit_time-time_start}')\n",
    "print(classification_report(y_test_heart, sa_nn_epochs.predict(X_test_heart)))\n",
    "print(classification_report(y_train_heart, sa_nn_epochs.predict(X_train_heart)))\n",
    "plt.plot(sa_nn_epochs.fitness_curve[:,0],label='Training')\n",
    "sa_train_geo_100 = sa_nn_epochs.fitness_curve[:,0]\n",
    "sa_nn_epochs.fit(X_test_heart, y_test_heart)\n",
    "plt.plot(sa_nn_epochs.fitness_curve[:,0],label='Validation')\n",
    "sa_test_geo_100 = sa_nn_epochs.fitness_curve[:,0]\n",
    "plt.legend()\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(10,6)\n",
    "plt.xlabel(\"Epoch\", fontsize=18)\n",
    "plt.ylabel(\"Loss\", fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8db4245",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sa_nn_epochs = mlrose_hiive.NeuralNetwork(\n",
    "    hidden_nodes = [100,100], activation = 'relu', \n",
    "    algorithm = 'simulated_annealing', \n",
    "    max_iters=100000,\n",
    "    bias = True,\n",
    "    is_classifier = True, \n",
    "    learning_rate =  0.1,\n",
    "    early_stopping = True,\n",
    "    max_attempts = 100,\n",
    "    curve=True,\n",
    "    random_state=42,\n",
    "    schedule=mlrose_hiive.GeomDecay(300)\n",
    ")\n",
    "\n",
    "time_start = time.time()\n",
    "sa_nn_epochs.fit(X_train_heart, y_train_heart)\n",
    "fit_time = time.time()\n",
    "print(f'fit_time = {fit_time-time_start}')\n",
    "print(classification_report(y_test_heart, sa_nn_epochs.predict(X_test_heart)))\n",
    "print(classification_report(y_train_heart, sa_nn_epochs.predict(X_train_heart)))\n",
    "plt.plot(sa_nn_epochs.fitness_curve[:,0],label='Training')\n",
    "sa_train_geo_300 = sa_nn_epochs.fitness_curve[:,0]\n",
    "sa_nn_epochs.fit(X_test_heart, y_test_heart)\n",
    "plt.plot(sa_nn_epochs.fitness_curve[:,0],label='Validation')\n",
    "sa_test_geo_300 = sa_nn_epochs.fitness_curve[:,0]\n",
    "plt.legend()\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(10,6)\n",
    "plt.xlabel(\"Epoch\", fontsize=18)\n",
    "plt.ylabel(\"Loss\", fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b326d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sa_train_geo_1,  label=\"Temp = 1\")\n",
    "plt.plot(sa_train_geo_50, label ='Temp = 50')\n",
    "plt.plot(sa_train_geo_100, label ='Temp = 100')\n",
    "plt.plot(sa_train_geo_300, label ='Temp = 300')\n",
    "\n",
    "plt.xlabel(\"Epochs\"), plt.ylabel(\"Loss Values\"), plt.legend(loc=\"best\",prop={'size': 14})\n",
    "plt.title('Varying Tempeature - Fitness Scores', size = 16)\n",
    "plt.ylim([0.1,0.4])\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8e3b47",
   "metadata": {},
   "source": [
    "## MLP-ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0fd5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_heart = MLPClassifier(random_state=42, activation = \"relu\",alpha =0.001, hidden_layer_sizes=[100,100],solver='adam')\n",
    "train_df = pd.DataFrame([])\n",
    "val_df = pd.DataFrame([])\n",
    "for i in range(5):\n",
    "    X_train_train, X_test_train, y_train_train, y_test_train = train_test_split(X_train_heart, y_train_heart,\n",
    "                                                        test_size = 0.2, random_state = (i+1)*i, stratify = y_train_heart\n",
    "                                                       )\n",
    "    time_start = time.time()\n",
    "    pipeline_heart.fit(X_train_train, y_train_train)\n",
    "    fit_time = time.time()\n",
    "    print(f'fit_time = {fit_time-time_start}')\n",
    "    train_curve = pipeline_heart.loss_curve_\n",
    "    d = {i: train_curve}\n",
    "    loss_df = pd.DataFrame(data=d)\n",
    "    train_df = pd.concat([train_df,loss_df],1)\n",
    "    pipeline_heart.fit(X_test_train, y_test_train)\n",
    "    val_curve = pipeline_heart.loss_curve_\n",
    "    d = {i: val_curve}\n",
    "    loss_df = pd.DataFrame(data=d)\n",
    "    val_df = pd.concat([val_df,loss_df],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1160362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_df['Mean'] = train_df.loc[:, [0,1,2,3,4]].mean(axis = 1)\n",
    "val_df['Mean'] = val_df.loc[:, [0,1,2,3,4]].mean(axis = 1)\n",
    "ax = plt.gca()\n",
    "plt.plot(train_df['Mean'],  label=\"Training Error\")\n",
    "plt.plot(val_df['Mean'], label ='Validation Error')\n",
    "pipeline_heart.fit(X_test_heart, y_test_heart)\n",
    "plt.plot(pipeline_heart.loss_curve_, label ='Test Error')\n",
    "plt.title('Loss Curves - '+ 'Adam',fontsize = 16)\n",
    "plt.xlabel(\"Epochs\"), plt.ylabel(\"Loss Values\"), plt.legend(loc=\"best\",prop={'size': 14})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638f4d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_heart = MLPClassifier(random_state=42, activation = \"relu\",alpha =0.001, hidden_layer_sizes=[100,100],solver='adam')\n",
    "pipeline_heart.fit(X_train_heart, y_train_heart)\n",
    "y_pred = pipeline_heart.predict(X_test_heart)\n",
    "cm = confusion_matrix(y_test_heart, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=['No','Yes'],\n",
    "                             )\n",
    "disp.plot()\n",
    "disp.im_.colorbar.remove()\n",
    "plt.title('ADAM - Confusion Matrix')\n",
    "plt.show()\n",
    "print(classification_report(y_test_heart, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59934887",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "plt.plot(train_df['Mean'],  label=\"Training Error\")\n",
    "plt.plot(val_df['Mean'], label ='Validation Error')\n",
    "plt.title('Loss Curves - '+ 'Adam',fontsize = 16)\n",
    "plt.xlabel(\"Epochs\"), plt.ylabel(\"Loss Values\"), plt.legend(loc=\"best\",prop={'size': 14})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c33f53f",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8c505b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "plt.plot(train_df['Mean'],  label=\" Training - Adam\", color ='blue')\n",
    "plt.plot(val_df['Mean'], '--',  label=\" Val - Adam\", color ='blue')\n",
    "plt.plot(train_df_rhc['Mean'], label ='Training - RHC', color ='orange')\n",
    "plt.plot(val_df_rhc['Mean'], '--', label ='Val - RHC', color ='orange')\n",
    "plt.plot(train_df_ga['Mean'], label ='Training - GA', color ='green')\n",
    "plt.plot(val_df_ga['Mean'], '--', label ='Val - GA', color ='green')\n",
    "plt.plot(train_df_sa['Mean'], label ='Training - SA', color ='red')\n",
    "plt.plot(val_df_sa['Mean'], '--', label ='Val - SA', color ='red')\n",
    "plt.title('Loss Curves',fontsize = 16)\n",
    "plt.xlabel(\"Epochs\"), plt.ylabel(\"Loss Values\"), plt.legend(loc=\"upper right\",prop={'size': 12})\n",
    "plt.xscale('log',base=10) \n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
